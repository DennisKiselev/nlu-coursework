{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1GrEOjgZnlROaTa4zm3tdvcS_zXeAGF-q","timestamp":1713783396354}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","import sys\n","\n","drive.mount('/content/drive/')\n","cwk_dir =\"drive/MyDrive/NLU Coursework/\" #For running in Jack's Google Drive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gkZ1qIGo3SL3","executionInfo":{"status":"ok","timestamp":1713790366158,"user_tz":-60,"elapsed":1557,"user":{"displayName":"Jack Pay","userId":"06088299984570525080"}},"outputId":"8c0524ae-ae18-4029-db9d-75bdbc9dfbe4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["The model card template makes use of Jinja, hence we need to install the necessary package."],"metadata":{"id":"3y5__yaUKyb1"}},{"cell_type":"code","source":["!pip install Jinja2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kjSmXdzczFoh","executionInfo":{"status":"ok","timestamp":1713789468425,"user_tz":-60,"elapsed":7562,"user":{"displayName":"Jack Pay","userId":"06088299984570525080"}},"outputId":"23262f9d-7ee7-4120-b5c5-8d31feb7a58d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2) (2.1.5)\n"]}]},{"cell_type":"markdown","source":["Required import statement"],"metadata":{"id":"Puto8-5ILO2s"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"inUOAq0Yy_O5","executionInfo":{"status":"ok","timestamp":1713789468425,"user_tz":-60,"elapsed":5,"user":{"displayName":"Jack Pay","userId":"06088299984570525080"}}},"outputs":[],"source":["from huggingface_hub import ModelCard, ModelCardData"]},{"cell_type":"markdown","source":["Before running the cell below, upload the model card template (`COMP34812_modelcard_template.md`) provided to you using the Colab file browser (on the left-hand side)."],"metadata":{"id":"TX5nkStaLTRC"}},{"cell_type":"code","source":["card_data = ModelCardData(\n","    language='en',\n","    license='cc-by-4.0',\n","    tags=['text-classification'],\n","    # change the line below to specify the URL to your Github/Gitlab repo\n","    repo=\"https://github.com/DennisKiselev/nlu-coursework\",\n","    ignore_metadata_errors=True)\n","\n","#B\n","# card = ModelCard.from_template(\n","#     card_data = card_data,\n","#     template_path=f'{cwk_dir}/COMP34812_modelcard_template.md',\n","#     # change the following line to indicate your respective usernames\n","#     # and the abbreviation of the relevant track name, e.g., NLI, ED, AV\n","#     model_id = 'h61781jp-h37701dk-NLI',\n","\n","#     # the following lines were provided to give you an example value for each attribute\n","#     model_summary = '''This is a classification model that was trained to\n","#       detect whether one piece of text (the premise) semantically supports another (the hypothesis).''',\n","#     model_description = '''This model is based on XLNet embeddings, employs a Bi-LSTM architecture with subtractive & multiplicative sentence fusion, attention, dropout and LR scheduling.''', #B\n","#     developers = 'Jack Pay and Dennis Kiselev',\n","#     base_model_repo = 'https://huggingface.co/xlnet/xlnet-base-cased',\n","#     base_model_paper = 'https://arxiv.org/pdf/1906.08237.pdf',\n","#     model_type = 'Supervised',\n","#     model_architecture = 'Non-Transformer DNN',\n","#     language = 'English',\n","#     base_model = 'xlnet-base-cased',\n","#     training_data = '26K training premise-hypothesis pairs, and more than 6K validation pairs.',\n","#     hyperparameters = '''\n","#       - learning_rate: 1e-3\n","#       - learning_rate_scheduling: ReduceLROnPlateau\n","#       - scheduling_monitor: val_loss\n","#       - scheduling_factor: 0.1\n","#       - scheduling_min_delta: 0.01\n","#       - scheduling_min_lr: 1e-5\n","#       - scheduling_patience: 2\n","#       - train_batch_size: 256\n","#       - val_batch_size: 256\n","#       - num_epochs: 20\n","#       - dropout: 0.5\n","#       - loss_function: categorical_crossentropy\n","#       - optimizer: RMSprop\n","#       - prediction_activation_function: softmax\n","#       - embedding_trainable: False''',\n","#     speeds_sizes_times = '''\n","#       - overall training time: 5 minutes\n","#       - duration per training epoch: 16 seconds\n","#       - model size: 162.8MB''',\n","#     testing_data = '6K validation pairs.',\n","#     testing_metrics = '''\n","#       - Accuracy\n","#       - Precision\n","#       - Macro Precision\n","#       - Weighted Macro Precision\n","#       - Recall\n","#       - Macro Recall\n","#       - Weighted Macro Recall\n","#       - F1-Score\n","#       - Macro F1-Score\n","#       - Weighted Macro F1-Score\n","#       - MCC\n","#       - Loss''',\n","#     results = 'The model obtained an accuracy of 73%, weighted macro-precision of 73%, weighted macro-recall of 73%, weighted macro F1-score of 73%, MCC of 0.455 and loss of 0.54',\n","#     hardware_requirements = '''\n","#       - RAM: at least 8 GB\n","#       - Storage: at least 0.17GB,\n","#       - GPU: V100''',\n","#     software = '''\n","#       - keras: 2.15.0,\n","#       - transformers: 4.38.2,\n","#       - tensorflow: 2.15.0,\n","#       - numpy: 1.25.2,\n","#       - nltk: 3.8.1''',\n","#     bias_risks_limitations = '''Any premises over 110 tokens, or any hypotheses over 60 tokens will be truncated to these lengths. Sequences below these must be zero-padded.''',\n","#     additional_information = '''Various experimentation was conducted to find the optimal parameters settings for various hyperparameters, such as the size of certain layers.'''\n","# )\n","# # the following lines will write a markdown (.md) file; this becomes one of your model cards\n","# # change the filename accordingly\n","# with open(f'{cwk_dir}/solution_B/results_final/model_card.md', 'w') as model_card:\n","#   model_card.write(card.content)\n","\n","#C\n","card = ModelCard.from_template(\n","    card_data = card_data,\n","    template_path=f'{cwk_dir}/COMP34812_modelcard_template.md',\n","    # change the following line to indicate your respective usernames\n","    # and the abbreviation of the relevant track name, e.g., NLI, ED, AV\n","    model_id = 'h61781jp-h37701dk-NLI',\n","\n","    # the following lines were provided to give you an example value for each attribute\n","    model_summary = '''This is a classification model that was trained to\n","      detect whether one piece of text (the premise) semantically supports another (the hypothesis).''',\n","    model_description = '''This model is based upon a ROBERTA model finetuned upon 26K pairs of premises & hypotheses. This model utilised several data augmentation methods including synonym replacement and random word swapping. This model employed LR scheduling to improve performance.''', #C\n","    developers = 'Jack Pay and Dennis Kiselev',\n","    base_model_repo = 'https://huggingface.co/FacebookAI/roberta-base',\n","    base_model_paper = 'https://arxiv.org/pdf/1907.11692.pdf',\n","    model_type = 'Supervised',\n","    model_architecture = 'Transformers',\n","    language = 'English',\n","    base_model = 'roberta-base',\n","    training_data = '26K training premise-hypothesis pairs, and more than 6K validation pairs.',\n","    hyperparameters = '''\n","      - learning_rate: 2e-05\n","      - train_batch_size: 16\n","      - eval_batch_size: 16\n","      - optimizer: AdamW\n","      - num_epochs: 6\n","      - learning_rate_scheduling: get_linear_schedule_with_warmup\n","      - num_warmup_steps: 26944\n","      - num_training_steps: 161664''',\n","    speeds_sizes_times = '''\n","      - overall training time: 1 hour\n","      - duration per training epoch: 20 minutes\n","      - model size: 475.6MB''',\n","    testing_data = '6K validation pairs.',\n","    testing_metrics = '''\n","      - Accuracy\n","      - Precision\n","      - Macro Precision\n","      - Weighted Macro Precision\n","      - Recall\n","      - Macro Recall\n","      - Weighted Macro Recall\n","      - F1-Score\n","      - Macro F1-Score\n","      - Weighted Macro F1-Score\n","      - MCC\n","      - Loss''',\n","    results = 'The model obtained an accuracy of 88%, weighted macro-precision of 88%, weighted macro-recall of 88%, weighted macro F1-score of 88%, MCC of 0.760, and loss of 1.75',\n","    hardware_requirements = '''\n","      - RAM: at least 16 GB\n","      - Storage: at least 0.5GB,\n","      - GPU: V100''',\n","    software = '''\n","      - torch: 2.2.1+cu121''',\n","    bias_risks_limitations = '''Any inputs (concatenation of two sequences) longer than\n","      512 subwords will be truncated by the model.''',\n","    additional_information = '''Data augmentation methods included synonym replacement, synonym insertion, random word deletion, and random word swapping.'''\n",")\n","\n","# the following lines will write a markdown (.md) file; this becomes one of your model cards\n","# change the filename accordingly\n","with open(f'{cwk_dir}/solution_C/results/model_card.md', 'w') as model_card:\n","  model_card.write(card.content)"],"metadata":{"id":"pg4o6fuPbl5X","executionInfo":{"status":"ok","timestamp":1713790548789,"user_tz":-60,"elapsed":310,"user":{"displayName":"Jack Pay","userId":"06088299984570525080"}}},"execution_count":11,"outputs":[]}]}