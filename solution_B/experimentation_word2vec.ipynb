{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["6Z_nt6CDPmFL","5iRsx3jRavN_","Ar8n9iYLRqJ-","NNhHNF_7QCLR","7PIjaoLxQETl","W4JqJVPEaJ-F"],"gpuType":"V100","authorship_tag":"ABX9TyP2GPaQBxWeN0MN9ncCXx8u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["cwk_dir =\"drive/MyDrive/NLU Coursework/\" #For running in Jack's Google Drive"],"metadata":{"id":"OXxgT5_9Y2f5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"6Z_nt6CDPmFL"}},{"cell_type":"markdown","source":["## Connect Google Drive Folder"],"metadata":{"id":"5iRsx3jRavN_"}},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","import sys\n","drive.mount('/content/drive/')"],"metadata":{"id":"ueUCEpZiaxb6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"Ar8n9iYLRqJ-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xu6_MOGrOgk7"},"outputs":[],"source":["from tensorflow.keras import utils\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, Add, Layer, LSTM, Bidirectional, Embedding, concatenate, BatchNormalization, SimpleRNN, Attention, GlobalMaxPooling1D, GlobalAveragePooling1D, Conv1D, MaxPooling1D, TimeDistributed, subtract, multiply\n","from tensorflow.keras.optimizers.legacy import SGD, Adam, RMSprop\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.callbacks import Callback\n","from tensorflow.keras.optimizers.schedules import ExponentialDecay\n","from tensorflow.keras.metrics import SparseCategoricalAccuracy as Acc\n","from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy as KAcc\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.regularizers import l2\n","\n","import keras"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import typing\n","from PIL import Image\n","import json\n","from nltk.corpus import stopwords\n","import gensim.downloader as api\n","from gensim.models import Word2Vec\n","import nltk\n","from nltk.tokenize import word_tokenize\n","import string\n","from random import randint"],"metadata":{"id":"9stsorj3fgDx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel"],"metadata":{"id":"a_HjaQUzRIBR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Imports from local libraries\n","sys.path.append(cwk_dir)\n","from classes.evaluation import evaluate\n","from classes.preprocessing import load_data"],"metadata":{"id":"CnXQlybKUtwm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Primary Variables"],"metadata":{"id":"NNhHNF_7QCLR"}},{"cell_type":"markdown","source":["Filepath variables"],"metadata":{"id":"2QuJjCaZSGKQ"}},{"cell_type":"code","source":["solution_dir = os.path.join(cwk_dir, \"solution_B\")\n","models_dir = os.path.join(solution_dir, \"models\")\n","results_dir = os.path.join(solution_dir, \"results\")"],"metadata":{"id":"Ca4vAm0rshSC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preprocessing variables"],"metadata":{"id":"V9ZpgIEce2m_"}},{"cell_type":"code","source":["LOWER: bool = True\n","PADDING: str = \"post\"\n","\n","nltk.download('stopwords')\n","# STOP_WORDS = set(stopwords.words('english'))\n","STOP_WORDS = []"],"metadata":{"id":"wD-vIHW0e4JC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training variables"],"metadata":{"id":"MCtnUhDpVdGS"}},{"cell_type":"code","source":["INITIAL_LR: float = 2e-5\n","EPOCHS: int = 40\n","VALIDATION_SPLIT: float = 0.2\n","BATCH_SIZE: int = 256\n","\n","DROPOUT: float = 0.25\n","\n","OPTIMIZER = RMSprop(INITIAL_LR)"],"metadata":{"id":"reO2X7MfVer7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Other"],"metadata":{"id":"Ae8SvQ7sVsZL"}},{"cell_type":"code","source":["MAX_PREMISE_LENGTH: int = None\n","MAX_HYPOTHESIS_LENGTH: int = None\n","\n","VOCAB_SIZE: int = None #None is the value to denote that there is no vocab size yet. This is set later, once we have the training data\n","EMBEDDING_SIZE: int = None"],"metadata":{"id":"o3eLxvA4VtPT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Functions"],"metadata":{"id":"7PIjaoLxQETl"}},{"cell_type":"code","source":["def average_sequence_length(sequence: typing.List[str]) -> int:\n","  \"\"\"\n","  Takes a list of sentences & returns the average length of sentences in that sequence\n","  \"\"\"\n","  lengths = [len(sample) for sample in sequence]\n","  return int(np.sum(lengths)/len(lengths)) + 1"],"metadata":{"id":"Wx7DF0zeeqAO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_model_architecture(model: Model, filename: str) -> None:\n","  \"\"\"\n","  Takes a model architecture and will a diagram of the architecture. Saves this diagram also, to the filename specified\n","  \"\"\"\n","  model.summary()\n","  filename = os.path.join(results_dir, filename)\n","  plot_model(model, to_file=filename)\n","  img = Image.open(filename)\n","  fig, ax = plt.subplots(figsize=(15, 15))\n","  plt.imshow(img, aspect='equal')"],"metadata":{"id":"lFtwPpokZA-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_data(tokenizer: Tokenizer, premises: typing.List[str], hypotheses: typing.List[str], maxlen: int = None) ->typing.Tuple[typing.List[str], typing.List[str]]:\n","  \"\"\"\n","  Uses the input tokenizer to tokenizer the premises & hypotheses. Will then pad the sequences correctly, using the maxlen passed in\n","  \"\"\"\n","  premises = tokenizer.texts_to_sequences(premises)\n","  hypotheses = tokenizer.texts_to_sequences(hypotheses)\n","\n","  premises = pad_sequences(premises, maxlen=MAX_PREMISE_LENGTH, padding=PADDING)\n","  hypotheses = pad_sequences(hypotheses, maxlen=MAX_HYPOTHESIS_LENGTH, padding=PADDING)\n","\n","  return (premises, hypotheses)"],"metadata":{"id":"AXdTexuHfnr-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_keras_model(model: Model, filename: str) -> None:\n","  \"\"\"\n","  Saves the model that's been made\n","  \"\"\"\n","  model.save_weights(os.path.join(models_dir, f\"{filename}.hdf5\"))\n","  model_architecture = model.to_json()\n","  with open(os.path.join(models_dir, f\"{filename}.json\"), \"w\") as f:\n","    f.write(json.dumps(model_architecture, indent=4))"],"metadata":{"id":"ubniOIi-sIJh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_sentences(sentences: typing.List[str]) -> typing.List[str]:\n","  \"\"\"\n","  Takes a list of sentences & cleans them. Remove stopwords, reduces to lower case, removes punctuation\n","  \"\"\"\n","  sentences = [[word.lower().replace(string.punctuation,\"\") for word in sentence.split(\" \") if word not in STOP_WORDS] for sentence in sentences]\n","  sentences = [\" \".join(sentence) for sentence in sentences]\n","  return sentences\n"],"metadata":{"id":"3PyD8IgR8qfX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_history(history, export_path, legend: typing.List[str] = ['train', 'test']):\n","  \"\"\"\n","  Will plot the history of a model, labelling it appropriately\n","  \"\"\"\n","  plt.figure(figsize=(20, 10))\n","  plt.subplot(1, 2, 1)\n","\n","  plt.plot(history.history['accuracy'])\n","  plt.plot(history.history['val_accuracy'])\n","  plt.title('model accuracy')\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epoch')\n","  plt.legend(legend, loc='upper left')\n","\n","  plt.subplot(1, 2, 2)\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('model loss')\n","  plt.ylabel('loss')\n","  plt.xlabel('epoch')\n","  plt.legend(legend, loc='upper left')\n","\n","  plt.savefig(os.path.join(results_dir, export_path))\n","\n","  plt.show()\n"],"metadata":{"id":"t-ctea8YAD3d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"tmplgEhQQGa9"}},{"cell_type":"markdown","source":["## Load Data"],"metadata":{"id":"47YHR_RvQKOH"}},{"cell_type":"code","source":["(train_premises, train_hypotheses, train_labels), (dev_premises, dev_hypotheses, dev_labels) = load_data(cwk_dir)"],"metadata":{"id":"lQwHWpA_QLJ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_PREMISE_LENGTH: int = average_sequence_length(sequence=train_premises)\n","MAX_HYPOTHESIS_LENGTH: int = average_sequence_length(sequence=train_hypotheses)\n","\n","print(f\"MAX_PREMISE_LENGTH: {MAX_PREMISE_LENGTH}\")\n","print(f\"MAX_HYPOTHESIS_LENGTH: {MAX_HYPOTHESIS_LENGTH}\")"],"metadata":{"id":"R132KHuqekFP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_labels = np.array(train_labels).reshape(len(train_labels),1)\n","dev_labels = np.array(dev_labels).reshape(len(dev_labels),1)"],"metadata":{"id":"mCMCOuTGYOTA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Removes stop words, punctuation, reduces to lower case\n","train_premises = clean_sentences(sentences=train_premises)\n","train_hypotheses = clean_sentences(sentences=train_hypotheses)\n","\n","dev_premises = clean_sentences(sentences=dev_premises)\n","dev_hypotheses = clean_sentences(sentences=dev_hypotheses)"],"metadata":{"id":"W6-dSfMA8nwP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Example"],"metadata":{"id":"sTZitr0QZ__L"}},{"cell_type":"code","source":["index = randint(0, len(train_premises))\n","print(f\"Premise: {train_premises[index]}\")\n","print(f\"Hypothesis: {train_hypotheses[index]}\")\n","print(f\"Label: {train_labels[index]}\")"],"metadata":{"id":"mTE8n_QhZ8B2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tokenize Data"],"metadata":{"id":"mqj3-EkCamIR"}},{"cell_type":"code","source":["tokenizer = Tokenizer(lower=LOWER) #Automatically sets sentence to lower & removes punctuation\n","tokenizer.fit_on_texts(train_premises + train_hypotheses + dev_premises + dev_hypotheses)"],"metadata":{"id":"_mHVdTUoRcY0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_premises, train_hypotheses = tokenize_data(tokenizer=tokenizer, premises=train_premises, hypotheses=train_hypotheses)\n","dev_premises, dev_hypotheses = tokenize_data(tokenizer=tokenizer, premises=dev_premises, hypotheses=dev_hypotheses)"],"metadata":{"id":"em9n7xwPbuuI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["VOCAB_SIZE = len(tokenizer.word_index) + 1\n","print(f\"Vocabulary size: {VOCAB_SIZE}\")"],"metadata":{"id":"02cuinRLeI-H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Example"],"metadata":{"id":"MuO-c_ej_Pp7"}},{"cell_type":"code","source":["#Premise\n","print(f\"Sentence: {tokenizer.sequences_to_texts([train_premises[index]])}\")\n","print(f\"Tokens: {train_premises[index]}\")"],"metadata":{"id":"PG-vJe_876in"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Hypothesis\n","print(f\"Sentence: {tokenizer.sequences_to_texts([train_hypotheses[index]])}\")\n","print(f\"Tokens: {train_hypotheses[index]}\")"],"metadata":{"id":"iQXuGniOcWG7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Embedding Matrix"],"metadata":{"id":"xpqcX_5Et5l8"}},{"cell_type":"code","source":["### Will use word2vec to create an embedding matrix\n","EMBEDDING_SIZE = 300\n","embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_SIZE))\n","\n","nltk.download('punkt')\n","word2vec = api.load('word2vec-google-news-300')\n","\n","for word, index in tokenizer.word_index.items():\n","  try:\n","      embedding_matrix[index, :] = word2vec[word]\n","  except KeyError:\n","    pass\n","print(f\"Embeddings shape: {embedding_matrix.shape}\")"],"metadata":{"id":"aKoj8M8pWaMt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiment: Word2Vec Embeddings"],"metadata":{"id":"T5P9yW21YYd3"}},{"cell_type":"code","source":["model_name = \"word2vec_embed\""],"metadata":{"id":"UHIMfRALm2pc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Architecture"],"metadata":{"id":"IJOW-8bZYZ8w"}},{"cell_type":"code","source":["input_premises = Input(shape=(MAX_PREMISE_LENGTH,))\n","input_hypotheses = Input(shape=(MAX_HYPOTHESIS_LENGTH,))\n","\n","embedding_layer = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_SIZE, weights=[embedding_matrix], trainable=True)\n","premises = embedding_layer(input_premises)\n","hypotheses = embedding_layer(input_hypotheses)\n","\n","lstm_layer = Bidirectional(LSTM(512, return_sequences=False, dropout=DROPOUT))\n","premises = lstm_layer(premises)\n","hypotheses = lstm_layer(hypotheses)\n","\n","merged = concatenate([premises, hypotheses], axis=-1)\n","\n","output_layer = Dense(1, activation='sigmoid')(merged)\n","\n","model = Model(inputs=[input_premises, input_hypotheses], outputs=output_layer)\n","model.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n","\n","show_model_architecture(model=model, filename=f\"{model_name}_architecture.png\")"],"metadata":{"id":"ksIf9ABn0al0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training"],"metadata":{"id":"W4JqJVPEaJ-F"}},{"cell_type":"code","source":["history = model.fit([train_premises, train_hypotheses], train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT)"],"metadata":{"id":"boPFGqRKaL16"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_history(history=history, export_path=f\"{model_name}_architecture.pdf\")"],"metadata":{"id":"pGZZLw8sAjKY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save the model\n","save_keras_model(model=model, filename=\"model_bert_embed\")"],"metadata":{"id":"FyJtJdIssmDJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Evaluation"],"metadata":{"id":"QaqeotdSaMPX"}},{"cell_type":"code","source":["#Get loss\n","loss, _ = model.evaluate([dev_premises, dev_hypotheses], dev_labels)\n","print(\"\\nTest score/loss:\", loss)"],"metadata":{"id":"KlMpBkcicAOB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predicted_labels = model.predict(x=[dev_premises, dev_hypotheses])\n","predicted_labels = (predicted_labels > 0.5).astype(int)\n","\n","#Evaluation Metrics\n","test_metrics = evaluate(true_labels=np.array(dev_labels), predicted_labels=np.array(predicted_labels))\n","test_metrics.to_csv(os.path.join(results_dir, f\"{model_name}_metrics.csv\"), index=False)\n","test_metrics.head()"],"metadata":{"id":"28szS3zhaNn8"},"execution_count":null,"outputs":[]}]}